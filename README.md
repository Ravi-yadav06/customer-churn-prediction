ğŸ“Š Customer Churn Prediction System

A Machine Learning project to predict whether a customer is likely to churn (leave the service) based on historical customer data. The system helps businesses identify at-risk customers and take preventive actions.

ğŸš€ Project Overview

Customer churn is a critical business problem. Retaining existing customers is cheaper than acquiring new ones. This project builds a classification model that predicts churn using customer behavioral and demographic data.

ğŸ›  Tech Stack

Python

Pandas & NumPy â€“ Data preprocessing

Scikit-learn â€“ Model training and evaluation

Matplotlib / Seaborn â€“ Visualization

FastAPI (optional) â€“ Model deployment

Jupyter Notebook / Google Colab

ğŸ“ Project Structure
Customer-Churn/
â”‚
â”œâ”€â”€ churn_model.ipynb
â”œâ”€â”€ dataset.csv
â”œâ”€â”€ model.pkl
â”œâ”€â”€ main.py (optional API)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ“Œ Problem Statement

Predict whether a customer will:

0 â†’ Not Churn

1 â†’ Churn

Based on features such as:

Customer tenure

Monthly charges

Contract type

Payment method

Internet services

Demographic information

âš™ Data Preprocessing Steps

The following preprocessing steps were applied:

Handling missing values

Encoding categorical variables

Feature selection

Train-test split

Handling class imbalance

Feature scaling (for applicable models)

ğŸ¤– Models Used

The following machine learning models were trained and compared:

Logistic Regression

Decision Tree

Random Forest

AdaBoost

Final model selected based on performance metrics and imbalance handling.

âš– Handling Imbalanced Dataset

The dataset was imbalanced (more non-churn customers than churn customers). To address this:

Class Weight Balancing was applied

Threshold Tuning was performed

PR-AUC was used as the main evaluation metric

ğŸ“ˆ Evaluation Metrics

Due to data imbalance, accuracy alone was not sufficient. The following metrics were used:

Precision

Recall

F1-Score

Confusion Matrix

PR-AUC (Primary Metric)

ROC-AUC (Secondary)

ğŸ“Š Sample Results

Example evaluation output:

Accuracy: ~78%

PR-AUC: ~0.61

Improved recall after threshold tuning

These results indicate moderate but meaningful performance in identifying churn customers.

ğŸ¯ Threshold Tuning

Instead of using default probability threshold (0.5), threshold tuning was applied to:

Improve churn recall

Reduce false negatives

Optimize business impact

This allowed better identification of high-risk customers.

ğŸ§  Key Learnings

Accuracy is misleading for imbalanced datasets

PR-AUC is more reliable than ROC-AUC for churn prediction

Threshold tuning improves minority class detection

Feature engineering impacts model performance significantly

â–¶ How To Run The Project
1ï¸âƒ£ Install Dependencies
pip install pandas numpy scikit-learn matplotlib seaborn fastapi uvicorn

2ï¸âƒ£ Run Notebook

Open:

churn_model.ipynb


in Jupyter Notebook or Google Colab.

3ï¸âƒ£ (Optional) Run API Server
uvicorn main:app --reload


Open:

http://127.0.0.1:8000/docs

ğŸ“Œ Business Impact

This system can help companies:

Identify customers at risk

Reduce churn rate

Improve customer retention

Increase revenue

ğŸš€ Future Improvements

Hyperparameter tuning

SMOTE oversampling

Deep learning models

Model deployment on cloud

Real-time prediction dashboard

ğŸ‘¨â€ğŸ’» Author

Ravi
Machine Learning Developer
JECRC University
